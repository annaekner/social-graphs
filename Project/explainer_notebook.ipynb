{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Project**  \n",
    "**02805 Social Graphs & Interactions**  \n",
    "\n",
    "**Group 13** \n",
    "- Anna Bøgevang Ekner (s193396)\n",
    "- Morten Møller Christensen (s204258)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspiration**: https://paperswithcode.com/dataset/reddit  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodes are posts, and there is a link between two posts if the same user has commented on both. \n",
    "\n",
    "50 random subreddits, each with 20 top/hot posts from this month.\n",
    "\n",
    "Each node (post) should have the attributes:\n",
    "\n",
    "- Title of the post\n",
    "- Upvotes of the post\n",
    "- Text of the post\n",
    "- Subreddit name that the post belongs to\n",
    "- Usernames of comments on that post\n",
    "\n",
    "Filter posts with a text below some number of words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import praw\n",
    "import json\n",
    "import warnings\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "sns.set(font_scale=1.)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1: Scraping the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Part 1.1: Setting up the Reddit API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_API_tokens():\n",
    "    # Load environment variables from the .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    # Retrieve tokens from environment variables\n",
    "    client_id = os.getenv('CLIENT_ID')\n",
    "    client_secret = os.getenv('CLIENT_SECRET')\n",
    "    user_agent = os.getenv('USER_AGENT')\n",
    "\n",
    "    return client_id, client_secret, user_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API tokens\n",
    "client_id, client_secret, user_agent = load_API_tokens()\n",
    "\n",
    "# Initialize Reddit instance with credentials from the .env file\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Part 1.2: Scraping and saving the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only subreddits with a minimum number of subscribers are chosen. \\\n",
    "Top posts, and only posts with non-empty text are chosen. \n",
    "\n",
    "n_subreddits = 50, n_posts_per_subreddit = 20. \\\n",
    "Each subreddit is saved as a .json file that contains:\n",
    "\n",
    "Subreddit information: \n",
    "- Name\n",
    "- Description\n",
    "- Subscribers\n",
    "- Posts\n",
    "\n",
    "Post information: \n",
    "- Title\n",
    "- Author\n",
    "- Score\n",
    "- Text\n",
    "- Subreddit\n",
    "- URL\n",
    "- ID\n",
    "- Comments\n",
    "\n",
    "Comment information:\n",
    "- Author\n",
    "- Score\n",
    "- Text\n",
    "- ID\n",
    "\n",
    "All of this information is stored in case it could become interesting to use in our exploratory analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reddit_data(n_subreddits, n_posts_per_subreddit, min_subscribers, top_all, data_dir):\n",
    "    \"\"\"\n",
    "    Scrape Reddit data for a given number of random subreddits and save the data to JSON files.\n",
    "\n",
    "    For {n_subreddits} random subreddits with at least {min_subscribers} subscribers, scrapes the \n",
    "    top {n_posts_per_subreddit} posts and comments from the past year and saves the data to JSON files.\n",
    "\n",
    "    Args \n",
    "        n_subreddits: Number of random subreddits to fetch (int)\n",
    "        n_posts_per_subreddit: Number of top posts to fetch per subreddit (int)\n",
    "        min_subscribers: Minimum number of subscribers for a subreddit to be included (int)\n",
    "        top_all: Time frame for top posts (e.g., 'all', 'year', 'month', 'week', 'day', 'hour')\n",
    "        data_dir: Directory to save the JSON files (str)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of subreddit JSON files already saved\n",
    "    saved_subreddits = len(os.listdir(data_dir))\n",
    "\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    while saved_subreddits < n_subreddits:\n",
    "\n",
    "        # Fetch a random subreddit and validate its subscriber count\n",
    "        random_subreddit = reddit.subreddit(\"random\")\n",
    "        if random_subreddit.subscribers < min_subscribers:\n",
    "            continue\n",
    "\n",
    "        print(f\"Selected subreddit: {random_subreddit.display_name} with {random_subreddit.subscribers} subscribers\")\n",
    "\n",
    "        # Fetch top posts and initialize data structure\n",
    "        limit = n_posts_per_subreddit * 1000\n",
    "        top_posts = random_subreddit.top(top_all, limit=limit)\n",
    "\n",
    "        subreddit_data = {\n",
    "            \"subreddit\": random_subreddit.display_name,\n",
    "            \"description\": random_subreddit.public_description,\n",
    "            \"subscribers\": random_subreddit.subscribers,\n",
    "            \"posts\": []\n",
    "        }\n",
    "\n",
    "        # Counter for the number of posts without empty text\n",
    "        saved_posts = 0\n",
    "\n",
    "        while saved_posts < n_posts_per_subreddit and len(subreddit_data[\"posts\"]) < n_posts_per_subreddit:\n",
    "\n",
    "            # Get the next top post\n",
    "            post = next(top_posts)\n",
    "\n",
    "            # Skip posts with empty text\n",
    "            if not post.selftext.strip():\n",
    "                continue\n",
    "\n",
    "            post_data = {\n",
    "                \"title\": post.title,\n",
    "                \"author\": post.author.name if post.author else \"deleted\",\n",
    "                \"score\": post.score,\n",
    "                \"text\": post.selftext,\n",
    "                \"subreddit\": post.subreddit.display_name,\n",
    "                \"url\": post.url,\n",
    "                \"id\": post.id,\n",
    "                \"comments\": []\n",
    "            }\n",
    "\n",
    "            # Fetch up to 10 comments per post\n",
    "            post.comments.replace_more(limit=0)\n",
    "            post_data[\"comments\"] = [\n",
    "                {\n",
    "                    \"comment_id\": comment.id,\n",
    "                    \"author\": comment.author.name if comment.author else \"deleted\",\n",
    "                    \"body\": comment.body,\n",
    "                    \"score\": comment.score\n",
    "                }\n",
    "                for comment in post.comments.list()\n",
    "            ]\n",
    "\n",
    "            saved_posts += 1\n",
    "            subreddit_data[\"posts\"].append(post_data)\n",
    "\n",
    "        # Save subreddit data to a JSON file\n",
    "        file_path = os.path.join(data_dir, f\"{random_subreddit.display_name}.json\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(subreddit_data, f, indent=4)\n",
    "\n",
    "        print(f\"Data saved to {file_path}\\n\")\n",
    "        saved_subreddits += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected subreddit: GTA6 with 1109910 subscribers\n",
      "Data saved to data\\GTA6.json\n",
      "\n",
      "Selected subreddit: mycology with 723386 subscribers\n",
      "Data saved to data\\mycology.json\n",
      "\n",
      "Selected subreddit: KGBTR with 732132 subscribers\n",
      "Data saved to data\\KGBTR.json\n",
      "\n",
      "Selected subreddit: legal with 224324 subscribers\n",
      "Data saved to data\\legal.json\n",
      "\n",
      "Selected subreddit: Welding with 454792 subscribers\n",
      "Data saved to data\\Welding.json\n",
      "\n",
      "Selected subreddit: NetflixBestOf with 10516850 subscribers\n",
      "Data saved to data\\NetflixBestOf.json\n",
      "\n",
      "Selected subreddit: CryptoCurrency with 8896475 subscribers\n",
      "Data saved to data\\CryptoCurrency.json\n",
      "\n",
      "Selected subreddit: SCP with 736094 subscribers\n",
      "Data saved to data\\SCP.json\n",
      "\n",
      "Selected subreddit: AskCulinary with 1041501 subscribers\n",
      "Data saved to data\\AskCulinary.json\n",
      "\n",
      "Selected subreddit: help with 1217691 subscribers\n",
      "Data saved to data\\help.json\n",
      "\n",
      "Selected subreddit: rap with 2200361 subscribers\n",
      "Data saved to data\\rap.json\n",
      "\n",
      "Selected subreddit: starwarsmemes with 560048 subscribers\n",
      "Data saved to data\\starwarsmemes.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scrape and save Reddit data\n",
    "scrape_reddit_data(n_subreddits = 50, \n",
    "                   n_posts_per_subreddit = 20, \n",
    "                   min_subscribers = 200000,\n",
    "                   top_all = \"year\", \n",
    "                   data_dir = \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Part 1.3: Extracting user comment info of each post**\n",
    "Possibly save a .txt file for each post that contains the usernames of the comments? \\\n",
    "To make it easier to look for intersections between commenters of different posts, when creating the links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2: Building the graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(data_dir):\n",
    "    \"\"\"\n",
    "    Build the Reddit undirected graph from the scraped data. Nodes are posts, and a \n",
    "    link between two nodes is created when the same user comments on both posts.\n",
    "\n",
    "    Args\n",
    "        data_dir: Directory containing the subreddit JSON files (str)\n",
    "\n",
    "    Returns \n",
    "        G: Undirected graph representing the Reddit network (nx.Graph)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes for each post\n",
    "    for file_name in os.listdir(data_dir):\n",
    "        with open(os.path.join(data_dir, file_name), \"r\", encoding=\"utf-8\") as f:\n",
    "            subreddit_data = json.load(f)\n",
    "\n",
    "        for post in subreddit_data[\"posts\"]:\n",
    "            G.add_node(post[\"id\"], \n",
    "                       title = post[\"title\"], \n",
    "                       subreddit = post[\"subreddit\"], \n",
    "                       text = post[\"text\"], \n",
    "                       comments = post[\"comments\"])\n",
    "\n",
    "    # List of all nodes\n",
    "    nodes_list = list(G.nodes(data=True))\n",
    "\n",
    "    # Add links based on shared commenters\n",
    "    for i, post1 in enumerate(nodes_list):\n",
    "        for post2 in nodes_list[i+1:]:\n",
    "\n",
    "            # Get the comments for each post\n",
    "            post1_comments = post1[1][\"comments\"]\n",
    "            post2_comments = post2[1][\"comments\"]\n",
    "\n",
    "            # Get the usernames of the commenters for each post\n",
    "            post1_commenters = set(comment[\"author\"] for comment in post1_comments)\n",
    "            post2_commenters = set(comment[\"author\"] for comment in post2_comments)\n",
    "\n",
    "            # Find the common commenters between the two posts\n",
    "            common_commenters = set(post1_commenters).intersection(post2_commenters)\n",
    "\n",
    "            if common_commenters:\n",
    "                G.add_edge(post1[0], post2[0])\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of nodes: 1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the Reddit graph\n",
    "G = create_graph(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of nodes: 1000\n",
      "Total number of edges: 409740\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Total number of edges: {G.number_of_edges()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course02506",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
